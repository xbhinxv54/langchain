{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m a large language model, trained by Google.  I don\\'t have a physical body, a personal life, or emotions.  My purpose is to process information and respond to a wide range of prompts and questions. I do this by drawing on a massive dataset of text and code that I was trained on.  This dataset includes things like books, articles, code, and conversations.\\n\\nWhile I can generate human-like text in response to a wide range of prompts and questions, it\\'s important to remember that I don\\'t actually \"understand\" things in the same way a human does.  I don\\'t have beliefs, opinions, or consciousness.  Instead, I identify patterns in the data I was trained on and use those patterns to generate text that is relevant to the given prompt.\\n\\nI can be used for a variety of tasks, such as:\\n\\n* **Answering questions:** I can provide information on a wide range of topics.\\n* **Generating creative content:** I can write stories, poems, articles, and other types of text.\\n* **Translating languages:** I can translate text between multiple languages.\\n* **Summarizing text:** I can condense longer pieces of text into shorter summaries.\\n* **Writing different kinds of creative content:**  This includes poems, code, scripts, musical pieces, email, letters, etc.\\n\\nWhile I strive to be informative and comprehensive in my responses, it\\'s important to be aware of my limitations.  I can sometimes generate incorrect or biased information, and I\\'m still under development.  Therefore, it\\'s always a good idea to double-check information I provide, especially for important decisions.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e79bf603-6c45-4f4c-8ce6-136fb2236b91-0', usage_metadata={'input_tokens': 5, 'output_tokens': 347, 'total_tokens': 352, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "llm.invoke(\"Tell me about yourself\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(input_variables=[\"topic\"],template=\"Explain {topic} in a single sentence.\")\n",
    "llm_chain=prompt|llm\n",
    "response=llm_chain.invoke({\"topic\":\"quantum computing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#Other types withsummary,with k window memory\n",
    "\n",
    "\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str):\n",
    "    if session_id not in store:\n",
    "        store[session_id]=InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "chain=RunnableWithMessageHistory(llm,get_session_history)\n",
    "\n",
    "res1=chain.invoke(\"Hi I am abhinav\",config={\"configurable\": {\"session_id\": \"1\"}})\n",
    "res2=chain.invoke(\"Hi I am john\",config={\"configurable\": {\"session_id\": \"2\"}})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is Abhinav.  Do you have any other questions?\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is my name\",config={\"configurable\": {\"session_id\": \"1\"}}).content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is John.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is my name\",config={\"configurable\": {\"session_id\": \"2\"}}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Supervised Learning',\n",
       " 'Unsupervised Learning',\n",
       " 'Reinforcement Learning',\n",
       " 'Semi-supervised Learning',\n",
       " 'Deep Learning']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.list import CommaSeparatedListOutputParser\n",
    "output_parser=CommaSeparatedListOutputParser()\n",
    "format_instructions=output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {topic}.\\n\\n{format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "result=chain.invoke({\"topic\":\"machine learning\"})\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structred Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.structured import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "\n",
    "# Define the structure you want\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"name\", description=\"The name of the programming language\"),\n",
    "    ResponseSchema(name=\"year\", description=\"Year the language was created\"),\n",
    "    ResponseSchema(name=\"creator\", description=\"Who created the language\")\n",
    "]\n",
    "\n",
    "# Create the parser\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Get the format instructions\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Create prompt template with format instructions using partial variables\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Provide information about {language}.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Please provide your response:\"\"\",\n",
    "    input_variables=[\"language\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# Create and run chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"language\": \"Python\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Python', 'year': '1991', 'creator': 'Guido van Rossum'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: content='* **Qubits and Superposition:** Unlike regular bits which are either 0 or 1, qubits can exist in a superposition, being both 0 and 1 simultaneously, like a spinning coin before it lands.\\n* **Entanglement:** Multiple qubits can be linked through entanglement, meaning their states are correlated, even when separated.  Knowing the state of one entangled qubit reveals information about the others.\\n* **Quantum Speedup:**  These properties allow quantum computers to explore many possibilities at once, making them potentially much faster than classical computers for specific complex tasks like drug discovery, materials science, financial modeling, and cryptography.\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-17c44ec3-5cb5-4e2c-be4c-0b246e3182aa-0' usage_metadata={'input_tokens': 361, 'output_tokens': 131, 'total_tokens': 492, 'input_token_details': {'cache_read': 0}}\n",
      "Original Explanation: Imagine a regular computer bit like a light switch: it can be either ON (1) or OFF (0).  A quantum bit, or qubit, is like a special light switch that can be ON, OFF, or *both at the same time*. This \"both at the same time\" state is called superposition.\n",
      "\n",
      "Think of it like flipping a coin. While it's spinning in the air, it's both heads and tails *potentially*, but you don't know which until it lands. A qubit is like that spinning coin before it lands.\n",
      "\n",
      "Another key idea is entanglement.  This is where two or more qubits become linked, even if they're far apart.  It's like having two of those spinning coins, but magically knowing that if one lands on heads, the other *must* land on tails, even before they touch the ground.\n",
      "\n",
      "Because qubits can be in multiple states at once, and because they can be entangled, quantum computers can explore many possibilities *simultaneously*.  This allows them to tackle certain types of problems much faster than regular computers, like:\n",
      "\n",
      "* **Drug discovery:** Simulating molecules to design new drugs.\n",
      "* **Materials science:** Designing new materials with specific properties.\n",
      "* **Financial modeling:** Creating more accurate and complex financial models.\n",
      "* **Cryptography:** Breaking existing encryption and creating new, more secure methods.\n",
      "\n",
      "It's important to note that quantum computers are not meant to replace regular computers. They are good at different things. They are specialized tools for solving specific kinds of complex problems that are currently intractable for even the most powerful supercomputers.  They are still in their early stages of development, but hold immense potential for the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence,RunnableLambda\n",
    "\n",
    "# Define the prompts\n",
    "prompt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Explain {topic} in simple terms.\")\n",
    "prompt2 = PromptTemplate(input_variables=[\"text\"], template=\"Summarize this {text} in three bullet points.\")\n",
    "\n",
    "\n",
    "chain = (\n",
    "    prompt1 \n",
    "    | llm \n",
    "    | RunnableLambda(lambda x: {\n",
    "        \"summary\": (prompt2 | llm).invoke({\"text\": x.content}),\n",
    "        \"original\": x.content\n",
    "    })\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"topic\": \"quantum computing\"})\n",
    "\n",
    "# Output the result\n",
    "print(\"Summary:\", result[\"summary\"])\n",
    "print(\"Original Explanation:\", result[\"original\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "explaination_prompt=PromptTemplate(input_variables=[\"topic\"],template=\"\"\"Provide a detailed explanation of the following topic: {topic}\n",
    "    Your explanation should be comprehensive and clear.\"\"\")\n",
    "\n",
    "key_points_prompt=PromptTemplate(input_variables=[\"explanation\"],template=\"\"\"Given the following explanation, extract 3-5 key points that capture the most important aspects:\n",
    "    \n",
    "    {explanation}\n",
    "    \n",
    "    Format your response as a JSON object with a single key 'key_points' containing an array of strings.\"\"\")\n",
    "\n",
    "\n",
    "class KeyPointsOutputParser(JsonOutputParser):\n",
    "    def parse(self,text):\n",
    "        try:\n",
    "            parsed=json.loads(text)\n",
    "            return parsed[\"key_points\"]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "explaination_chain=explaination_prompt|llm\n",
    "key_points_chain=key_points_prompt|llm|KeyPointsOutputParser()\n",
    "\n",
    "\n",
    "explanation=explaination_chain.invoke({\"topic\":\"GenAI\"})\n",
    "\n",
    "key_points=key_points_chain.invoke({\"explanation\":explanation.content})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid=[x for x in key_points[\"key_points\"]][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GenAI creates new content like text, images, audio, and code by learning patterns from training data, unlike traditional AI that classifies or predicts.',\n",
       " 'Key GenAI models include GANs (competing generator and discriminator), VAEs (latent space representation), Transformers (attention mechanism), and Diffusion Models (noise reversal).']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Runnable: Invalid input: Input must be between 0 and 10\n",
      "Error in Runnable: Invalid output: invalid literal for int() with base 10: 'None\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "class SafeRun(Runnable):\n",
    "    def __init__(self,func) -> None:\n",
    "        self.func=func\n",
    "    def invoke(self,input_data,config=None):\n",
    "        try:\n",
    "            return self.func(input_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Runnable: {e}\")\n",
    "            return None\n",
    "    \n",
    "def numerical(input_data):\n",
    "    if isinstance(input_data, dict):\n",
    "        number = input_data.get(\"number\")\n",
    "        try:\n",
    "            if int(number) > 10 or int(number) < 0:\n",
    "                raise ValueError(\"Input must be between 0 and 10\")\n",
    "            return input_data  # Return original dict to maintain chain flow\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Invalid input: {e}\")\n",
    "def numerical_2(message):\n",
    "    try:\n",
    "        if hasattr(message, 'content'):  # If it's an AIMessage\n",
    "            content = message.content\n",
    "        else:\n",
    "            content = str(message)\n",
    "            \n",
    "        result = int(content)  # Try to convert to int\n",
    "        if result > 0:  # Add any validation you need\n",
    "            return str(result)  # Convert back to string for chain output\n",
    "        else:\n",
    "            raise ValueError(\"Output must be a positive integer\")\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid output: {e}\")\n",
    "\n",
    "\n",
    "prompt=PromptTemplate(input_variables=[\"number\"],template=\"\"\"\n",
    "                        Multiply the number {number} with any number of your choice and return the product alone\n",
    "                        IMPORTANT:Just respond with the result do not add anything\n",
    "                      \"\"\")\n",
    "input_check=SafeRun(numerical)\n",
    "output_check=SafeRun(numerical_2)\n",
    "\n",
    "chain=input_check|prompt|llm|output_check\n",
    "\n",
    "chain.invoke({\"number\":\"10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
